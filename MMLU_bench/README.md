This folder contrains scripts for analyzing meta-in-context learning (mil) from LLMs on the [MMLU benchmark](https://arxiv.org/pdf/2009.03300v3.pdf).

# Running scripts:
## 1. meta_in_context.py
This script runs meta-in-context learning (and standard in-context learning) on the MMLU benchmark for a given LLM. It takes the following arguments:
```
--engine: The name and storing name path of the LLM to use.
--num-meta-tasks: The number of meta-tasks to be trained on before testing mil performance.
--num-points-per-meta-task: The number of points to sample for each meta-task.
--num-shots-last-task: The number of shots to use for the test task.
```

## 2. meta_in_context_only_stem.py
Same as above but subselecting the STEM supercategories tasks.

## 3. matched_in_context_stem.py
Only runs in-context learning on the STEM supercategories tasks but matching the number of shots used in the meta-in-context learning setting. E.g. if the meta-in-context learning setting uses 3 shots of 3 different tasks, the matched in-context learning setting will use 9 shots of the test task before evaluation.

# Plotting scripts:
## 1. plot_iclVSmil.py
Plots the results of performance of meta-in-context learning vs. in-context learning on the MMLU benchmark (Figure 9.A in the paper). It also plots the results of matched in-context learning on the STEM supercategories tasks as an upper bound for the performance of meta-in-context learning in the extreme case where the meta-tasks are identical to the test task.

## 2. plot_similarity_reg.py
Plots the effect of Task similarity and Trial number on the performance of meta-in-context learning on the MMLU benchmark (Figures 9.B & 9.C in the paper).

## Directory Structure:
data/: Contains the MMLU benchmark data.

results/: Contains the results of running the LLMs on the MMLU benchmark.

task_sims/: Contains the task similarites for the engines which ran meta-in-context learning on the MMLU benchmark.

plots/: Contains plots generated by the plotting scripts.